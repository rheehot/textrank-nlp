{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data import Field, BucketIterator, RawField\n",
    "from torchtext.vocab import GloVe\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/judepark/Documents/paper_projects/text_rank/venv/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "SRC = Field(tokenize=lambda x: x.split(\" \"), lower=True, batch_first=True)\n",
    "TRG = Field(tokenize=lambda x: x.split(\" \"), init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/judepark/Documents/paper_projects/text_rank/venv/lib/python3.7/site-packages/torchtext/data/example.py:13: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "\n",
    "dataset = TabularDataset(path='../rsc/preprocessed/kp20k.valid_100_lines.json',\n",
    "                         format='json',\n",
    "                         fields={\n",
    "                                 'doc_words':('text', SRC), \n",
    "                                 'keyphrases': ('label', TRG)}\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'investigate', 'the', 'problem', 'of', 'delay', 'constrained', 'maximal', 'information', 'collection', 'for', 'csma', 'based', 'wireless', 'sensor', 'networks', '.', 'we', 'study', 'how', 'to', 'allocate', 'the', 'maximal', 'allowable', 'transmission', 'delay', 'at', 'each', 'node', ',', 'such', 'that', 'the', 'amount', 'of', 'information', 'collected', 'at', 'the', 'sink', 'is', 'maximized', 'and', 'the', 'total', 'delay', 'for', 'the', 'data', 'aggregation', 'is', 'within', 'the', 'given', 'bound', '.', 'we', 'formulate', 'the', 'problem', 'by', 'using', 'dynamic', 'programming', 'and', 'propose', 'an', 'optimal', 'algorithm', 'for', 'the', 'optimal', 'assignment', 'of', 'transmission', 'attempts', '.', 'based', 'on', 'the', 'analysis', 'of', 'the', 'optimal', 'solution', ',', 'we', 'propose', 'a', 'distributed', 'greedy', 'algorithm', '.', 'it', 'is', 'shown', 'to', 'have', 'a', 'similar', 'performance', 'as', 'the', 'optimal', 'one', '.']\n"
     ]
    }
   ],
   "source": [
    "print(vars(dataset[0])['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['algorithms', '__;__', 'performance', '__;__', 'data', 'aggregation', '__;__', 'sensor', 'networks']\n"
     ]
    }
   ],
   "source": [
    "print(vars(dataset[0])['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SRC.build_vocab(dataset, vectors=GloVe(name='6B', dim=100))\n",
    "TRG.build_vocab(dataset, vectors=GloVe(name='6B', dim=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 2648\n",
      "Unique tokens in target (en) vocabulary: 439\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_iterator = BucketIterator(\n",
    "    dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 305,    4, 2296,  849,    4,  155,  131,   14,  346,    3,  659,    3,\n",
      "           5,  390, 1420,    6,   62, 1037,   13,    6,   83, 1920,   65, 1209,\n",
      "          78,   41,   44,   22,   16,    8, 2137,  316,   17, 1624, 2171,   65,\n",
      "        2417,  131,   17,   18, 1072,    7, 1793,    6, 1065,    7,  557,    3,\n",
      "          15,   28, 1674,    8,  144, 2540,  121,    4,  155,  390,   17, 2616,\n",
      "         557,   21,    8,  300,    4,   62, 1309,    4, 1897, 1444,    6,  155,\n",
      "         131,  317,  432,  362,    6, 1534, 1065,    7, 1372,  970,    3,  390,\n",
      "         386, 2532, 1084,   23,  155,  131,   18,  461,   31,    2, 2220,    4,\n",
      "        1547,    6, 1599,    3,   15,  197,  390, 1389,    7, 2037, 1630,   65,\n",
      "           2,  432,  362,    4,  155,  131,    6,   93,   54,  290, 2348, 1019,\n",
      "          33, 2082, 2556, 2595,  863,    3,    8, 2533,    4, 1309,    4,  155,\n",
      "         131,   10,   35,    6, 1501,    9, 2048, 1356,   18,  207,    3,    2,\n",
      "          35,  121,   26,   25,   58,   11, 1253,  362,   68,  155,  131,    5,\n",
      "         582,    4, 2265,   88,   30,    5,  921,    9, 1716,    5, 2340, 1041,\n",
      "           5,    6, 1077, 1273,  582,    4,  131,    3,   54,  344,  155,  362,\n",
      "          18, 2617,   20,    2,   98,    4,   38,  345,    3,    2,  121,   26,\n",
      "         297,    8,   69,  326,   11, 1098,  155,  984,    9, 1209,   92,    5,\n",
      "         566,  557,    6, 1648,  390, 2352,   46,    3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "for i, batch in enumerate(train_iterator):\n",
    "    print(batch.text[0])\n",
    "#     print(' '.join([SRC.vocab.itos[i.item()] for i in batch.text[0]]))\n",
    "#     print(' '.join([TRG.vocab.itos[i.item()] for i in batch.label[0]]))\n",
    "    \n",
    "#     batch.text = torch.einsum('ij->ji', batch.text)\n",
    "#     batch.label = torch.einsum('ij->ji', batch.label)\n",
    "    \n",
    "#     print(batch.text.shape)\n",
    "#     print(' '.join([SRC.vocab.itos[i.item()] for i in batch.text[0]]))\n",
    "#     print(' '.join([TRG.vocab.itos[i.item()] for i in batch.label[0]]))\n",
    "    \n",
    "    break\n",
    "#     print(' '.join([SRC.vocab.itos[i.item()] for i in batch.text[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.keyword.data.graph_util import build_graph, normalize_graph\n",
    "\n",
    "def batch_graph(grhs):\n",
    "    \"\"\" batch a list of graphs\n",
    "    @param grhs: list(tensor,...) \n",
    "    \"\"\"\n",
    "    b = len(grhs)  # batch size\n",
    "    graph_dims = [len(g) for g in grhs]\n",
    "    s = max(graph_dims)  # max seq length\n",
    "    \n",
    "    G = torch.zeros([b, s, s])\n",
    "    for i, g in enumerate(grhs):\n",
    "        s_ = graph_dims[i]\n",
    "        G[i,:s_,:s_] = g\n",
    "    return G\n",
    "\n",
    "def build_graph_dataset(dataset: TabularDataset):\n",
    "    GRH = RawField(postprocessing=batch_graph)\n",
    "\n",
    "    for d in tqdm(dataset):\n",
    "        token_len = len(d.text)\n",
    "        G = build_graph(token_len, token_len)\n",
    "        A_f = G['forward']\n",
    "        A_b = G['backward']\n",
    "        d.A_f = normalize_graph(A_f)\n",
    "        d.A_b = normalize_graph(A_b)\n",
    "\n",
    "    dataset.fields['A_f'] = GRH\n",
    "    dataset.fields['A_b'] = GRH\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/judepark/Documents/paper_projects/text_rank/venv/lib/python3.7/site-packages/torchtext/data/field.py:36: UserWarning: RawField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "100%|██████████| 84/84 [00:57<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = build_graph_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1601, 0.1602, 0.0802,  ..., 0.0024, 0.0027, 0.0038],\n",
      "        [0.0000, 0.1604, 0.1605,  ..., 0.0025, 0.0027, 0.0038],\n",
      "        [0.0000, 0.0000, 0.1606,  ..., 0.0025, 0.0028, 0.0039],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.4000, 0.4472, 0.3162],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5000, 0.7071],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]])\n",
      "torch.Size([107, 107])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].A_f)\n",
    "print(dataset[0].A_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/judepark/Documents/paper_projects/text_rank/venv/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_iterator = Iterator(\n",
    "    dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_graph(grhs):\n",
    "    \"\"\" batch a list of graphs\n",
    "    @param grhs: list(tensor,...) \n",
    "    \"\"\"\n",
    "    b = len(grhs)  # batch size\n",
    "    graph_dims = [len(g) for g in grhs]\n",
    "    s = max(graph_dims)  # max seq length\n",
    "    \n",
    "    G = torch.zeros([b, s, s])\n",
    "    for i, g in enumerate(grhs):\n",
    "        s_ = graph_dims[i]\n",
    "        G[i,:s_,:s_] = g\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 244, 244])\n",
      "torch.Size([8, 244])\n",
      "torch.Size([244])\n",
      "torch.Size([244])\n",
      "torch.Size([244])\n",
      "torch.Size([244])\n",
      "torch.Size([244])\n",
      "torch.Size([244])\n",
      "torch.Size([244])\n",
      "torch.Size([244])\n",
      "torch.Size([244, 244])\n",
      "torch.Size([244, 244])\n",
      "torch.Size([244, 244])\n",
      "torch.Size([244, 244])\n",
      "torch.Size([244, 244])\n",
      "torch.Size([244, 244])\n",
      "torch.Size([244, 244])\n",
      "torch.Size([244, 244])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "#     print(batch_graph(batch.A_f).shape)\n",
    "#     print(batch_graph(batch.A_f)[-1])\n",
    "    print(batch.A_f.shape)\n",
    "    print(batch.text.shape)\n",
    "    for j in range(BATCH_SIZE):\n",
    "        print(batch.text[j].shape)\n",
    "    for j in range(BATCH_SIZE):\n",
    "        print(batch.A_f[j].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:47<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "SRC = Field(tokenize=lambda x: x.split(\" \"), lower=True, batch_first=True)\n",
    "TRG = Field(tokenize=lambda x: x.split(\" \"), init_token='<sos>', eos_token='<eos>', lower=True, batch_first=True)\n",
    "\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = TabularDataset.splits(\n",
    "    path='../rsc/preprocessed',\n",
    "    train='kp20k.train_100_lines.json',\n",
    "    validation='kp20k.valid_100_lines.json',\n",
    "    test='kp20k.test_100_lines.json',\n",
    "    format='json',\n",
    "    fields={\n",
    "        'doc_words':('text', SRC), \n",
    "        'keyphrases': ('label', TRG)\n",
    "    }\n",
    ")\n",
    "\n",
    "train_dataset = build_graph_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(dataset)\n",
    "TRG.build_vocab(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(list(train_dataset), './train_dataset.pt')\n",
    "# torch.save(SRC, 'SRC.data')\n",
    "# torch.save(TRG, 'TRG.vocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill in /Users/judepark/Documents/paper_projects/text_rank/venv/lib/python3.7/site-packages (0.3.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "\n",
    "with open('./train_dataset.pkl', 'wb') as f:\n",
    "    dill.dump(list(train_dataset), f)\n",
    "    f.close()\n",
    "    \n",
    "with open('./SRC.pkl', 'wb') as f:\n",
    "    dill.dump(SRC, f)\n",
    "    f.close()\n",
    "\n",
    "with open('./TRG.pkl', 'wb') as f:\n",
    "    dill.dump(TRG, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./SRC.pkl', 'rb') as f:\n",
    "    src_data = dill.load(f)\n",
    "    f.close()\n",
    "\n",
    "with open('./TRG.pkl', 'rb') as f:\n",
    "    trg_data = dill.load(f)\n",
    "    f.close()\n",
    "    \n",
    "with open('./train_dataset.pkl', 'rb') as f:\n",
    "    loaded_dataset = dill.load(f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRH = RawField(postprocessing=None)\n",
    "\n",
    "data_fields = [('text', src_data), ('label', trg_data), ('A_f', GRH), ('A_b', GRH)]\n",
    "\n",
    "load = Dataset(loaded_dataset, data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/judepark/Documents/paper_projects/text_rank/venv/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: Iterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "it = Iterator(\n",
    "        dataset=load,\n",
    "        batch_size=4,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 4]\n",
      "\t[.text]:[torch.LongTensor of size 4x196]\n",
      "\t[.label]:[torch.LongTensor of size 4x11]\n",
      "\t[.A_f]:[tensor([[0.1459, 0.1460, 0.0730,  ..., 0.0013, 0.0014, 0.0020],\n",
      "        [0.0000, 0.1460, 0.1461,  ..., 0.0013, 0.0014, 0.0020],\n",
      "        [0.0000, 0.0000, 0.1461,  ..., 0.0013, 0.0014, 0.0020],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.4000, 0.4472, 0.3162],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5000, 0.7071],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]]), tensor([[0.1463, 0.1463, 0.0732,  ..., 0.0013, 0.0014, 0.0020],\n",
      "        [0.0000, 0.1464, 0.1464,  ..., 0.0013, 0.0014, 0.0020],\n",
      "        [0.0000, 0.0000, 0.1465,  ..., 0.0013, 0.0014, 0.0020],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.4000, 0.4472, 0.3162],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5000, 0.7071],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]]), tensor([[0.1538, 0.1539, 0.0770,  ..., 0.0018, 0.0020, 0.0029],\n",
      "        [0.0000, 0.1540, 0.1541,  ..., 0.0019, 0.0021, 0.0029],\n",
      "        [0.0000, 0.0000, 0.1542,  ..., 0.0019, 0.0021, 0.0029],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.4000, 0.4472, 0.3162],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5000, 0.7071],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]]), tensor([[0.1782, 0.1785, 0.0894,  ..., 0.0049, 0.0054, 0.0075],\n",
      "        [0.0000, 0.1788, 0.1791,  ..., 0.0050, 0.0055, 0.0077],\n",
      "        [0.0000, 0.0000, 0.1794,  ..., 0.0052, 0.0057, 0.0078],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.4000, 0.4472, 0.3162],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5000, 0.7071],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000]])]\n",
      "\t[.A_b]:[tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.7071, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3162, 0.4472, 0.4000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0020, 0.0014, 0.0013,  ..., 0.1461, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0014, 0.0013,  ..., 0.1461, 0.1460, 0.0000],\n",
      "        [0.0020, 0.0014, 0.0013,  ..., 0.0730, 0.1460, 0.1459]]), tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.7071, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3162, 0.4472, 0.4000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0020, 0.0014, 0.0013,  ..., 0.1465, 0.0000, 0.0000],\n",
      "        [0.0020, 0.0014, 0.0013,  ..., 0.1464, 0.1464, 0.0000],\n",
      "        [0.0020, 0.0014, 0.0013,  ..., 0.0732, 0.1463, 0.1463]]), tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.7071, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3162, 0.4472, 0.4000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0029, 0.0021, 0.0019,  ..., 0.1542, 0.0000, 0.0000],\n",
      "        [0.0029, 0.0021, 0.0019,  ..., 0.1541, 0.1540, 0.0000],\n",
      "        [0.0029, 0.0020, 0.0018,  ..., 0.0770, 0.1539, 0.1538]]), tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.7071, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.3162, 0.4472, 0.4000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0078, 0.0057, 0.0052,  ..., 0.1794, 0.0000, 0.0000],\n",
      "        [0.0077, 0.0055, 0.0050,  ..., 0.1791, 0.1788, 0.0000],\n",
      "        [0.0075, 0.0054, 0.0049,  ..., 0.0894, 0.1785, 0.1782]])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/judepark/Documents/paper_projects/text_rank/venv/lib/python3.7/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "for batch in it:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset\n",
    "\n",
    "# data_fields = [('text', SRC), ('label', TGT), ('A_f', GRH), ('A_b', GRH)]\n",
    "\n",
    "# dataset = (Dataset(torch.load('./train_dataset.pt'), data_fields))\n",
    "dataset = torch.load('./train_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = Iterator(\n",
    "        dataset=dataset,\n",
    "        batch_size=4,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'fields'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-473-05a5501841c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/paper_projects/text_rank/venv/lib/python3.7/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/paper_projects/text_rank/venv/lib/python3.7/site-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# copy field names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             self.input_fields = [k for k, v in dataset.fields.items() if\n\u001b[1;32m     29\u001b[0m                                  v is not None and not v.is_target]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'fields'"
     ]
    }
   ],
   "source": [
    "for batch in it:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
